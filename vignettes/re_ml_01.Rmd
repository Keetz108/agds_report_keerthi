---
title: "re_ml01.Rmd"
author: "Keerthi Divakaran"
date: "2025-05-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# installing packages in case not installed
use_pkgs <- c("dplyr", "caret", "tidyverse","knitr", "tidyr", "ggplot2", "rmarkdown", "rsample", "recipes", "Metrics", "FNN")  
new_pkgs <- use_pkgs[!(use_pkgs %in% installed.packages()[, "Package"])]
if (length(new_pkgs) > 0) install.packages(new_pkgs,repos = "http://cran.us.r-project.org")
invisible(lapply(use_pkgs, require, character.only = TRUE))

# loading required libraries
library(dplyr)
library(caret)
library(tidyverse)
library(knitr)
library(here)
library(tidyr)
library(ggplot2)
library(rmarkdown)
library(rsample)
library(recipes)
library(FNN)
library(Metrics)

```


```{r analysis 1}

# Comparison of the linear regression and KNN models

# 1 Load functions from external R script
source(paste(here(),"data/my_analysis_function.R", sep= "/"))

# 2 Interpretation below, outside this chunk

# 3 Visualising temporal variations of observed and modelled GPP for both models, covering all available dates.

# Generate predictions from both models
daily_fluxes$GPP_LM <- predict(mod_lm, newdata = daily_fluxes)
daily_fluxes$GPP_KNN <- predict(mod_knn, newdata = daily_fluxes)

# Convert TIMESTAMP to Date if necessary
daily_fluxes$TIMESTAMP <- as.Date(as.character(daily_fluxes$TIMESTAMP), format = "%Y%m%d")

# Gather the data for plotting
daily_fluxes_long <- daily_fluxes %>%
  select(TIMESTAMP, GPP_NT_VUT_REF, GPP_LM, GPP_KNN) %>%
  pivot_longer(cols = c(GPP_NT_VUT_REF, GPP_LM, GPP_KNN),
               names_to = "Model", values_to = "GPP")

# Plot temporal variations of observed and modeled GPP
ggplot(daily_fluxes_long, aes(x = TIMESTAMP, y = GPP, color = Model)) +
  geom_line() +
  labs(title = "Temporal Variations of Observed and Modeled GPP", 
       x = "Date", y = "GPP_NT_VUT_REF") + # Label the Y-axis as 'GPP_NT_VUT_REF'
  theme_minimal() +
  scale_color_manual(values = c("black", "blue", "red"), 
                     labels = c("Observed GPP", "Linear Regression", "KNN"))


```



```{r analysis 2}

# The role of k

# 1 Hypothesis formulation described outside the chunk
# 2 Putting hypothesis to test 
# splitting data and visualisation
# overfitting and underfitting regions in the graph
# f(k)= MAE


# 1. Load the dataset
daily_fluxes <- read.csv("../data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv")


# 2. Split the data into training and test sets
set.seed(1982)  # For reproducibility
split <- initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- training(split)
daily_fluxes_test <- testing(split)


# 3. Preprocessing recipe: using key predictors
pp <- recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
             data = daily_fluxes_train |> drop_na()) |> 
  step_BoxCox(all_predictors()) |> 
  step_center(all_numeric_predictors()) |>
  step_scale(all_numeric_predictors())


# 4. Train KNN model using 10-fold cross-validation
set.seed(1982)
mod_cv <- train(pp, 
                data = daily_fluxes_train |> drop_na(), 
                method = "knn",
                trControl = trainControl(method = "cv", number = 10),
                tuneGrid = data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100)),
                metric = "MAE")  # Evaluation metric, but Rsquared also reported

print(mod_cv)

# 5. Extract and reshape results for plotting
results_df <- mod_cv$results %>%
  select(k, MAE, Rsquared) %>%
  pivot_longer(cols = c("MAE", "Rsquared"),
               names_to = "Metric",
               values_to = "Value")


# 6. Plot MAE and RÂ² vs k
ggplot(results_df, aes(x = k, y = Value, color = Metric)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  scale_color_manual(values = c("MAE" = "red", "Rsquared" = "blue")) +
  labs(title = "Model Complexity (k) vs. MAE and R-squared",
       x = "Number of Neighbors (k)",
       y = "Metric Value",
       color = "Metric") +
  theme_minimal()


# 7. overfitting and underfitting regions in the graph from 6

# Function to compute test set MAE for a given k
get_test_mae <- function(k_value) {
  set.seed(1982)
  
  # Create pre-processing recipe
  pp <- recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, data = daily_fluxes_train) |>
    step_BoxCox(all_predictors()) |>
    step_center(all_numeric_predictors()) |>
    step_scale(all_numeric_predictors())
  
  # Fit KNN model
  mod <- train(
    pp,
    data = daily_fluxes_train |> drop_na(),
    method = "knn",
    tuneGrid = data.frame(k = k_value),
    trControl = trainControl(method = "none")  # no CV, we want to evaluate on test set
  )
  
  # Predict on test set
  preds <- predict(mod, newdata = daily_fluxes_test |> drop_na())
  
  # True values
  actual <- daily_fluxes_test |> drop_na() |> pull(GPP_NT_VUT_REF)
  
  # Compute MAE
  mae <- yardstick::mae_vec(truth = actual, estimate = preds)
  
  return(mae)
}



# 8. Example usage

get_test_mae(5)
get_test_mae(40)

k_values <- c(2, 5, 10, 20, 40, 60, 100)
test_maes <- sapply(k_values, get_test_mae)

plot_df <- data.frame(k = k_values, Test_MAE = test_maes)

ggplot(plot_df, aes(x = k, y = Test_MAE)) +
  geom_line() +
  geom_point() +
  labs(title = "Test MAE vs. k",
       x = "k (number of neighbors)",
       y = "Test MAE")



# 9 Finding optimal k

find_optimal_k <- function(k_values, train_data, test_data, predictors, response) {
  mae_test <- sapply(k_values, function(k) {
    model <- train(
      x = train_data[, predictors],
      y = train_data[[response]],
      method = "knn",
      tuneGrid = data.frame(k = k),
      trControl = trainControl(method = "none")
    )
    preds <- predict(model, newdata = test_data[, predictors])
    mean(abs(preds - test_data[[response]]))
  })
  optimal_k <- k_values[which.min(mae_test)]
  return(list(optimal_k = optimal_k, mae_test = mae_test))
}

# Usage:
results <- find_optimal_k(k_values = 1:40, train_data, test_data, predictors, response)
cat("Optimal k:", results$optimal_k, "\n")



knitr::opts_chunk$set(echo = TRUE)

```

# 2. Interpret observed differences in the context of the bias-variance trade-off:

# 2a. Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?

# lm Train value:R sq 0.67 and RMSE 1.58; Test value: R sq 0.67 and RMSE 1.6
# KNN Train value:R sq 0.77 and RMSE 1.31; Test value: R sq 0.72 and RMSE 1.49

# Interpretation: Linear regression model (lm) is not particularly flexible and it has only two parameters, slope and intercept, providing a straight line for the best fit between predictors and targets. Owing to its simplicity, there is a risk of not fully capturing the complexity of the data, and missing out the better precision of the interactions between predictors and targets. Unlike lm, K-Nearest Neighbours (KNN) model is a machine learning method which is extremely flexible that it provides information on the highly non-linear relationships and deal with interactions between large number of targets and predictors. This also bears the risk of overfitting.In the given data set run using lm  (train: R sq 0.67 and RMSE 0.58, test: R sq 0.67 and RMSE 0.6) and KNN (train: R sq 0.77 and RMSE 1.31, test: R sq 0.72 and RMSE 1.49), the larger values for the KNN model compared to lm could be attributed to the latter's better sensitivity in capturing the relationships between targets and predictors. This could also lead to overfitting. Although variance explained through R sq in KNN model suggest a better fit of 77% in train and 72% in test, the RMSE values 1.31 and 1.49 of train and test shows the possibility of overfitting.

# 2b. Why does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?

# Test set values (KNN model): R sq 0.72 and RMSE 1.49
# Test set values (lm model): R sq 0.67 and RMSE 1.6

# In the test set, KNN model, although having the risk of overfitting, has performed better than lm. R sq values have risen to 0.72 from 0.67 which captures better information about the variance in the test data therby improving the fit of the model. RMSE values, which measures the magnitude of errors in the predictions, improved from 1.6 to 1.49 in KNN model. This means the difference between the true and predicted values have reduced increasing the credibility of the KNN model. Thus, KNN model performs better on the test set as compared to the lm model.



# The role of k

# 1. State the hypothesis for how the R squared and the MAE evaluated on the test and training set would change for k approachng 1 and for k approaching N (without running the code). Hypothesis backed by bias-variance-trade-off concept. 
 
# Hypothesis: 
# R squared measures the proportion of variance or spread of the data in the dependent variable that is predicted from the independent variables (predictors). As k approaches 1, R squared decreases and can be highly sensitive affected by the noise of the single point resulting in overfitting. As k approaches N, R squared can initially improve because the model becomes less sensitive to noise. However, when it becomes too large, the model becomes too generalized decreasing the R squared as it fails to capture the underlying patterns in the data. As k approaches N, it smooth out decision boundary leading to underfitting. 
# Mean Absolute Error (MAE) is the average of the absolute differences between predicted and actual values.
# As k approaches 1, the MAE of the model might be low resulting in overfitting. As k approaches N, the MAE increases resulting in higher MAE resulting in underfitting. 


# 2. Testing hypothesis

# splitting data and visualisation
# overfitting and underfitting regions in the graph
# f(k)= MAE



```{r setup, include=FALSE}





knitr::opts_chunk$set(echo = TRUE)
```





```{r gpp-histogram, fig.width=6, fig.height=4}
daily_fluxes |> 
  ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + 
  geom_histogram()
```

```{r eval-model-plots }
eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```

```{r eval-model-plots }
ggplot(daily_fluxes_long, aes(x = TIMESTAMP, y = GPP, color = Model)) +
  geom_line() +
  labs(title = "Temporal Variations of Observed and Modeled GPP", 
       x = "Date", y = "GPP_NT_VUT_REF") + # Label the Y-axis as 'GPP_NT_VUT_REF'
  theme_minimal() +
  scale_color_manual(values = c("black", "blue", "red"), 
                     labels = c("Observed GPP", "Linear Regression", "KNN"))

```


```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
```


```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

---
title: "re_ml01.Rmd"
author: "Keerthi Divakaran"
date: "2025-05-04"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# installing packages in case not installed
use_pkgs <- c("dplyr", "caret", "tidyverse","knitr", "tidyr", "ggplot2", "rmarkdown", "rsample", "recipes", "Metrics", "FNN", "yardstick", "purrr")  
new_pkgs <- use_pkgs[!(use_pkgs %in% installed.packages()[, "Package"])]
if (length(new_pkgs) > 0) install.packages(new_pkgs,repos = "http://cran.us.r-project.org")
invisible(lapply(use_pkgs, require, character.only = TRUE))

# loading required libraries
library(dplyr)
library(caret)
library(tidyverse)
library(knitr)
library(here)
library(tidyr)
library(ggplot2)
library(rmarkdown)
library(rsample)
library(recipes)
library(FNN)
library(Metrics)
library(yardstick)
library(purrr)

```


```{r r_analysis1, echo=FALSE, message=FALSE, warning=FALSE}

# Comparison of the linear regression and KNN models

# 1 Load functions from external R script
source("../data/my_analysis_function.R")

# 2 Interpretation below, outside this chunk

# 3 Visualising temporal variations of observed and modelled GPP for both models, covering all available dates.

# Generate predictions from both models
daily_fluxes$GPP_LM <- predict(mod_lm, newdata = daily_fluxes)
daily_fluxes$GPP_KNN <- predict(mod_knn, newdata = daily_fluxes)

# Convert TIMESTAMP to Date if necessary
daily_fluxes$TIMESTAMP <- as.Date(as.character(daily_fluxes$TIMESTAMP), format = "%Y%m%d")

# Gather the data for plotting
daily_fluxes_long <- daily_fluxes %>%
  select(TIMESTAMP, GPP_NT_VUT_REF, GPP_LM, GPP_KNN) %>%
  pivot_longer(cols = c(GPP_NT_VUT_REF, GPP_LM, GPP_KNN),
               names_to = "Model", values_to = "GPP")

# Plot temporal variations of observed and modeled GPP
temporal_var <- ggplot(daily_fluxes_long, aes(x = TIMESTAMP, y = GPP, color = Model)) +
  geom_line() +
  labs(title = "Temporal Variations of Observed and Modeled GPP", 
       x = "Date", y = "GPP_NT_VUT_REF") + # Label the Y-axis as 'GPP_NT_VUT_REF'
  theme_minimal() +
  scale_color_manual(values = c("black", "blue", "red"), 
                     labels = c("Observed GPP", "Linear Regression", "KNN"))

print(temporal_var)

ggsave("../figures/re_ml_01_temporal_var_plot1.jpg", plot = temporal_var, width = 8, height = 6, dpi = 300)
```


```{r r_analysis2, echo=FALSE, message=FALSE, warning=FALSE }

# The role of k

# 1 Hypothesis formulation described outside the chunk
# 2 Putting hypothesis to test 
# splitting data and visualisation
# overfitting and underfitting regions in the graph
# f(k)= MAE


# 1. Load the dataset
daily_fluxes <- read.csv("../data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv")

# Data splitting
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)
df_train <- daily_fluxes_train |> drop_na()
df_test <- daily_fluxes_test |> drop_na()


# 2. Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(recipes::all_predictors()) |> 
  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
  recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())


# 3. Fit KNN model with a range of k values
k_values <- c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100, nrow(df_train))

mod_cv <- caret::train(pp, 
                       data = daily_fluxes_train |> drop_na(), 
                       method = "knn",
                       trControl = caret::trainControl(method = "cv", number = 10),
                       tuneGrid = data.frame(k = c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100)),
                       metric = "MAE")

mod_cv


# 4. Extract and reshape results for plotting
results_df <- mod_cv$results %>%
  select(k, MAE, Rsquared) %>%
  pivot_longer(cols = c("MAE", "Rsquared"),
               names_to = "Metric",
               values_to = "Value")


# 5. Optimal k values of both MAE and R squared
# optimal k with minimum MAE (optimal MAE)
optimal_k_mae <- mod_cv$results$k[which.min(mod_cv$results$MAE)]
optimal_k_mae

# optimal k with maximum R-squared (best fit)
optimal_k_r2 <- mod_cv$results$k[which.max(mod_cv$results$Rsquared)]
optimal_k_r2

cat("Optimal k based on training MAE:", optimal_k_mae, "\n")
cat("Optimal k based on training R-squared:", optimal_k_r2, "\n")


# 6. Plot MAE and RÂ² vs k

optimum_k <- ggplot(results_df, aes(x = k, y = Value, color = Metric)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  scale_color_manual(values = c("MAE" = "darkred", "Rsquared" = "blue")) +
  labs(title = "Model Complexity (k) vs. MAE and R-squared",
       x = "Number of Neighbors (k)",
       y = "Metric Value",
       color = "Metric") +
  theme_minimal()

print(optimum_k)

ggsave("../figures/re_ml_01_plot2.jpg", plot = optimum_k, width = 8, height = 6, dpi = 300)


# 7. Optimal k based on CV MAE
optimal_k_train <- mod_cv$results %>% 
  filter(MAE == min(MAE)) %>% 
  pull(k)

cat("Optimal k based on 10-fold CV (train set):", optimal_k_train, "\n")
optimal_k_train


# 8.  Function that takes k and returns test MAE
get_test_mae <- function(k) {
  mod <- caret::train(
    pp,
    data = df_train,
    method = "knn",
    trControl = caret::trainControl(method = "none"),
    tuneGrid = data.frame(k = k),
    metric = "MAE"
  )
  
  predictions <- predict(mod, newdata = df_test)
  mae <- mean(abs(predictions - df_test$GPP_NT_VUT_REF), na.rm = TRUE)
  
  return(data.frame(k = k, MAE_test = mae))
}


# 9. best k on test set
k_values <- c(2, 5, 10, 15, 20, 25, 30, 40, 60, 100)

test_results <- purrr::map_dfr(k_values, get_test_mae)

optimal_k_test <- test_results$k[which.min(test_results$MAE_test)]
cat("Optimal k on test set:", optimal_k_test)


# 10. visualisation
test_optimum_k <- ggplot(test_results, aes(x = k, y = MAE_test)) +
  geom_line(color = "darkred", size = 1) +
  geom_point(size = 2) +
  geom_vline(xintercept = 30, linetype = "dashed", color = "blue") +
  labs(title = "Test Set MAE vs k (Number of Neighbors)",
       x = "k (Number of Neighbors)",
       y = "Test MAE") +
  theme_minimal()

print(test_optimum_k)

ggsave("../figures/re_ml_01_plot3.jpg", plot = test_optimum_k, width = 8, height = 6, dpi = 300)

knitr::opts_chunk$set(echo = TRUE)
```

# 2. Observation: lm Train value:R sq 0.67 and RMSE 1.58; Test value: R sq 0.67 and RMSE 1.6. KNN Train value:R sq 0.77 and RMSE 1.31; Test value: R sq 0.72 and RMSE 1.49 (figs saved as re_ml_01_mod_lm and re_ml_01_mod_knn in the directory).
# Interpretation: Linear regression model (lm) is not particularly flexible and it has only two parameters, slope and intercept, providing a straight line for the best fit between predictors and targets. owing to its simplicity, there is a risk of not fully capturing the complexity of the data, and missing out the better precision of the interactions between predictors and targets. Unlike lm, K-Nearest Neighbours (KNN) model is a machine learning method which is extremely flexible that it provides information on the highly non-linear relationships and deal with interactions between large number of targets and predictors. This also bears the risk of overfitting.In the given data set run using lm  (train: R sq 0.67 and RMSE 0.58, test: R sq 0.67 and RMSE 0.6) and KNN (train: R sq 0.77 and RMSE 1.31, test: R sq 0.72 and RMSE 1.49), the larger values for the KNN model compared to lm could be attributed to the latter's better sensitivity in capturing the relationships between targets and predictors. This could also lead to overfitting. Although variance explained through R sq in KNN model suggest a better fit of 77% in train and 72% in test, the RMSE values 1.31 and 1.49 of train and test shows the possibility of overfitting.

# Test set values (KNN model): R sq 0.72 and RMSE 1.49
# Test set values (lm model): R sq 0.67 and RMSE 1.6
# Test set of KNN model, although having the risk of overfitting, has performed better than lm. R sq values have risen to 0.72 from 0.67 which captures better information about the variance in the test data therby improving the fit of the model. RMSE values, which measures the magnitude of errors in the predictions, improved from 1.6 to 1.49 in KNN model. This means the difference between the true and predicted values have reduced increasing the credibility of the KNN model. Thus, KNN model performs better on the test set as compared to the lm model.

# The role of k
# 1. Hypothesis: R squared measures the proportion of variance or spread of the data in the dependent variable that is predicted from the independent variables. As k approaches 1, R squared decreases and can be highly sensitive affected by the noise of the single point resulting in overfitting. As k approaches N, R squared can initially improve because the model becomes less sensitive to noise. However, when it becomes too large, the model becomes too generalized decreasing the R squared as it fails to capture the underlying patterns in the data. As k approaches N, it smooth out decision boundary leading to underfitting. 
#Mean Absolute Error (MAE) is the average of the absolute differences between predicted and actual values. As k approaches 1, the MAE of the model might be low resulting in overfitting. As k approaches N, the MAE increases resulting in underfitting.

# Underfitting happens when  the model is too simple to capture the underlying data patterns. In the plot underfitting happens as k increases leading to increase of MAE (both train and test). The model is smooth, ignoring important data points. Overfitting happens when the model is too complex and fits noise in the training data. In the plot, this happens where k is small (k= below 5), lowering MAE in train but high in test. At optimal k, the test MAE stays minimum.

# Model generalisability is evaluated across a range of k values using test set MAE as the performance metric. As the value of k increased, test MAE initially decreased, reaching a minimum at k = 30, beyond which performance plateaued or slightly worsened. This pattern reflects a classic tradeoff between bias and variance: lower k values risk overfitting, while excessively high k values lead to underfitting due to over-smoothing. The region around k = 30 represents a sweet spot where the model maintains a balance between capturing signal and avoiding noise. The lowest test MAE at k = 30 thus represents the best generalisation performance.
